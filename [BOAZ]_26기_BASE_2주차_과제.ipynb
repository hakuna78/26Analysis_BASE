{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 각자 이 ipynb 파일의 **사본을 생성**하여 과제 Q0~Q3까지 채운 후 해당 파일을 깃허브에 업로드해주세요!"
      ],
      "metadata": {
        "id": "YTOK1lA7Hsz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Sample Code in PyTorch"
      ],
      "metadata": {
        "id": "2msLr-G13YUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 직접 읽어보며 돌려볼 수 있는 **쉬운** 예제 코드~\n",
        "- 코드 간단 설명:\n",
        "  - 길이 12짜리 binary sequence(0/1)를 입력으로 받아서, 시퀀스 안에 1-0-1 pattern이 한 번이라도 등장하면 1, 아니면 0을 맞추는 binary classification을 수행하는 RNN 분류기\n",
        "  - 마지막에는 demo sequence로 예측 + hidden state 변화까지 출력하는 프로그램"
      ],
      "metadata": {
        "id": "5JrL9Fl93d0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "EO04CkcR4bFR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정답 label 만드는 함수\n",
        "- 역할:\n",
        "  - sequence(e.g., [1,0,1,0,0,...]) 안에 연속된 3칸이 1-0-1인 구간이 있는지 검사\n",
        "  - 있으면 label=1 / 없으면 label=0"
      ],
      "metadata": {
        "id": "TLXMSKiQ9k6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "rfrge_Gq8ZFx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습용 data 만드는 PatternDataset 클래스\n",
        "- 깨알 상식) PyTorch에서 Dataset은 \"데이터를 꺼내는 방식\"을 표준화한 클래스~\n",
        "  - 이걸 상속받아서 내가 하고자 하는 task에 부합하는 나만의 커스텀 Dataset 클래스를 만들어서 모델에 먹이는 겁니다 얍얍"
      ],
      "metadata": {
        "id": "OzaGLdAbGUSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset이 하는 일은 \"모델에 넣기 좋은 형태\"로 데이터를 제공하는 것!\n",
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        # (입력 시퀀스, 정답 label)을 n_samples개만큼 저장\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]  # 1) seq_len 길이의 random sequence 생성(0/1)\n",
        "            label = has_101_pattern(seq)                          # 2) has_101_pattern(seq)로 정답 label 생성\n",
        "            self.data.append((seq, label))                        # 3) (seq, label)을 self.data에 저장\n",
        "\n",
        "    # Dataset 안에 sample이 몇 개인지 알려주는 매직 메소드!\n",
        "    # 이걸로 보통 DataLoader가 \"전체 크기\"를 알 수 있게 합니다\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # idx번째 데이터를 꺼내서 pytorch tensor로 변환해주는 매직 메소드\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)         # (T,) = (12,) / embedding은 정수 인덱스를 받기 때문에 dtype으로 long을 사용합니다!\n",
        "        y = torch.tensor(label, dtype=torch.float32)    # scalar / BCEWithLogitsLoss가 float label(0.0/1.0)을 기대하는 편이라 dtype으로 float32를 사용했어요\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "4legZWrO8YJp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN 모델 클래스\n",
        "- pytorch에서 모델 클래스는 일반적으로 nn.Module을 상속해서 만들어요~"
      ],
      "metadata": {
        "id": "uTROYjWUINLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # 학습 연산을 위해 (0/1) -> '벡터'로 변환\n",
        "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True) # sequence를 왼쪽부터 읽으면서 hidden state를 update\n",
        "        self.fc = nn.Linear(hidden_dim, 1) # 마지막 hidden state로 이진 분류 점수(logit) 출력\n",
        "\n",
        "    # 일반 학습/평가용 feedforward 순전파 함수\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        NOTE: B는 batch size, T는 시퀀스의 길이!\n",
        "\n",
        "        input x: (B, T) 0/1 token\n",
        "        return: logits (B,)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (B, T, E) / 벡터화\n",
        "        out, h_n = self.rnn(emb)       # 각 시점의 hidden 기록인 out: (B, T, H) / 마지막 hidden state인 h_n: (1, B, H)\n",
        "        last_h = h_n[-1]               # (B, H) / 마지막 hidden만 추출\n",
        "        logits = self.fc(last_h)       # (B, H) -> (B, 1)\n",
        "        return logits.squeeze(1)       # (B,) / loss 계산 편하게 하기 위해 주로 이렇게 squeeze()라는 함수를 사용하여 모양을 맞춰줍니다\n",
        "\n",
        "    def forward_with_trace(self, x):\n",
        "        \"\"\"\n",
        "        시각화를 통해 이해할 수 있도록 time step별 hidden(out)과 마지막 예측(logits)을 함께 리턴하는 함수\n",
        "        x: (1, T) 단일 시퀀스만 넣는 것을 권장함\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (1, T, E)\n",
        "        out, h_n = self.rnn(emb)       # out: (1, T, H)\n",
        "        last_h = h_n[-1]               # (1, H)\n",
        "        logits = self.fc(last_h)       # (1, 1)\n",
        "        return logits.squeeze(1), out.squeeze(0)  # logits: (1,), out: (T, H)\n",
        "        # out.squeeze(0) 추가로 한 이유 : batch=1을 넣으면 shape이 (1,T,H)인데 이 batch의 차원(1)을 제거하여 (T,H)로 보기 좋게 만든것!\n",
        "        # (크게 중요한 건 아닌데 그냥 궁금하실까봐,,)"
      ],
      "metadata": {
        "id": "9B39dpKJ7Kl_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 메인 함수: train()\n",
        "- 내부 로직 STEP BY STEP 설명:\n",
        "  1. (train/val) dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model / loss function / optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증(val) 정확도 출력\n",
        "  6. 마지막에 demo sequence 1개 넣어서 확률 출력\n",
        "  7. demo sequence에서 시간별 hidden state 일부 출력"
      ],
      "metadata": {
        "id": "OXV7srCf9Jhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    \"\"\"\n",
        "    <헷갈리는 개념 (코드 지피티 딸깍하지 말고 이젠 꼭 알아두자)>\n",
        "    - Dataset: 데이터 1개를 어떻게 꺼낼지 정의\n",
        "    - DataLoader: 여러 개를 묶어서 batch를 만들고, 섞고, 반복 가능한 형태로 제공\n",
        "    - shuffle : train은 섞어서 학습이 안정적이고 (True) / val은 평가하는 거니까 섞을 필요 없음 (False)\n",
        "    \"\"\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # model/loss function/optimizer 준비\n",
        "    model = SimpleRNNClassifier(embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Epoch train loop\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()                                   # dropout/batch regularization 등의 mode들이 train 모드로 바뀜\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:                       # batch 단위로 (x, y) 받음\n",
        "            x, y = x.to(device), y.to(device)           # x:(B,T), y:(B,)\n",
        "\n",
        "            logits = model(x)                           # (B,) / forward 실행\n",
        "            loss = criterion(logits, y)                 # scalar값 (정답 y와 예측 점수인 logit을 비교하여 loss값 계산)\n",
        "\n",
        "            optimizer.zero_grad()                       # 이전 gradient를 0으로\n",
        "            loss.backward()                             # backpropagation\n",
        "            optimizer.step()                            # parameter update\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()                                    # 평가 모드\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():                           # 평가이므로 학습 때와 달리 gradient 계산 하지 않음\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs >= 0.5).float()          # 0.5 이상이면 1로 예측하도록 구현\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # DEMO: hidden state 흐름을 출력해보자!\n",
        "    # ----------------------------\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0]   # 패턴 101이 있는 입력 시퀀스\n",
        "    demo_seq1 = [1,0,1,0,0,0,0,0,0,0,0,0]\n",
        "    demo_seq2 = [1,1,0,0,1,1,0,0,1,1,0,0]\n",
        "    demo = torch.tensor([demo_seq2], dtype=torch.long).to(device)  # (1,T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit, h_trace = model.forward_with_trace(demo)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq2)\n",
        "    print(\"Final prob(pattern=101):\", round(prob, 4))\n",
        "\n",
        "    # hidden trace 일부 출력(앞 3스텝 + 마지막 3스텝)\n",
        "    h_cpu = h_trace.cpu()  # (T,H)\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    for t in list(range(3)) + list(range(len(demo_seq2)-3, len(demo_seq2))):\n",
        "        vec = h_cpu[t][:6].tolist()  # hidden_dim 16 중 앞 6개만 보기\n",
        "        vec = [round(v, 3) for v in vec]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq2[t]} -> h_t[:6]={vec}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHlsjUj8Ntd",
        "outputId": "346b22dd-8d19-458b-f7c6-5bf6c9d24f72"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5957  val_acc=0.7750\n",
            "[Epoch 2] train_loss=0.5752  val_acc=0.7750\n",
            "[Epoch 3] train_loss=0.5396  val_acc=0.8030\n",
            "[Epoch 4] train_loss=0.5100  val_acc=0.8060\n",
            "[Epoch 5] train_loss=0.4721  val_acc=0.8390\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
            "Final prob(pattern=101): 0.7662\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[-0.506, 0.438, 0.112, 0.793, 0.54, 0.339]\n",
            "t= 1, x_t=1 -> h_t[:6]=[-0.609, 0.938, 0.064, 0.854, 0.885, 0.636]\n",
            "t= 2, x_t=0 -> h_t[:6]=[0.601, 0.92, -0.397, 0.584, 0.85, -0.591]\n",
            "t= 9, x_t=1 -> h_t[:6]=[-0.636, 0.913, 0.345, 0.952, 0.903, 0.452]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.668, 0.946, -0.296, 0.65, 0.924, -0.678]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.729, -0.421, 0.257, 0.743, 0.004, -0.955]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 코드에서 demo_seq 변수를 아래 두 가지로 바꿔서 각각 실행해보세요~\n",
        "  - 패턴 있음: [1,0,1,0,0,0,0,0,0,0,0,0] → 확률 높아야 함\n",
        "  - 패턴 없음: [1,1,0,0,1,1,0,0,1,1,0,0] → 확률 낮아야 함"
      ],
      "metadata": {
        "id": "IkUeto3kjsbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q0. 위 코드의 출력 결과 분석 & 두 가지 입력을 넣었을 때 각각의 결과를 비교 분석하시오."
      ],
      "metadata": {
        "id": "QBSysnq3kNjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)\n",
        "\n",
        "[패턴 있는 경우]\n",
        "```\n",
        "[Epoch 1] train_loss=0.6076  val_acc=0.7220\n",
        "[Epoch 2] train_loss=0.5598  val_acc=0.7220\n",
        "[Epoch 3] train_loss=0.5201  val_acc=0.7480\n",
        "[Epoch 4] train_loss=0.5026  val_acc=0.7620\n",
        "[Epoch 5] train_loss=0.4567  val_acc=0.7830\n",
        "\n",
        "=== Demo ===\n",
        "Sequence: [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "Final prob(pattern=101): 0.2414\n",
        "\n",
        "Hidden state trace (show first 3 and last 3 time steps):\n",
        "t= 0, x_t=1 -> h_t[:6]=[0.717, 0.353, 0.357, 0.452, 0.035, -0.135]\n",
        "t= 1, x_t=0 -> h_t[:6]=[0.386, 0.729, 0.902, -0.249, -0.507, 0.161]\n",
        "t= 2, x_t=1 -> h_t[:6]=[0.557, -0.486, 0.558, -0.278, 0.06, -0.137]\n",
        "t= 9, x_t=0 -> h_t[:6]=[-0.034, -0.261, -0.667, -0.076, -0.385, 0.118]\n",
        "t=10, x_t=0 -> h_t[:6]=[-0.163, -0.088, -0.754, 0.155, -0.426, 0.248]\n",
        "t=11, x_t=0 -> h_t[:6]=[-0.287, 0.134, -0.802, 0.375, -0.464, 0.372]\n",
        "```\n",
        "\n",
        "[패턴 없는 경우]\n",
        "```\n",
        "[Epoch 1] train_loss=0.6060  val_acc=0.7330\n",
        "[Epoch 2] train_loss=0.5659  val_acc=0.7370\n",
        "[Epoch 3] train_loss=0.5199  val_acc=0.7530\n",
        "[Epoch 4] train_loss=0.4973  val_acc=0.7660\n",
        "[Epoch 5] train_loss=0.3927  val_acc=0.9120\n",
        "\n",
        "=== Demo ===\n",
        "Sequence: [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
        "Final prob(pattern=101): 0.5186\n",
        "\n",
        "Hidden state trace (show first 3 and last 3 time steps):\n",
        "t= 0, x_t=1 -> h_t[:6]=[0.185, -0.036, 0.008, 0.962, 0.605, 0.853]\n",
        "t= 1, x_t=1 -> h_t[:6]=[0.584, -0.372, 0.325, 0.743, 0.323, 0.712]\n",
        "t= 2, x_t=0 -> h_t[:6]=[0.64, -0.004, 0.68, -0.677, -0.44, -0.666]\n",
        "t= 9, x_t=1 -> h_t[:6]=[0.631, 0.151, 0.174, 0.71, 0.682, 0.7]\n",
        "t=10, x_t=0 -> h_t[:6]=[0.788, 0.252, 0.327, -0.748, -0.124, -0.837]\n",
        "t=11, x_t=0 -> h_t[:6]=[-0.078, 0.055, 0.396, 0.702, -0.411, -0.59]\n",
        "\n",
        "```\n",
        "\n",
        "[결과 분석]\n",
        "\n",
        "패턴이 있는 seq1과 패턴이 없는 seq2의 결과에서 seq1의 정확도는 0.7830, seq2의 정확도는 0.9120으로 **seq2의 패턴 추출 성능이 더 좋음**을 확인할 수 있다.\n",
        "\n",
        "최종 확률 또한 seq1은 0.2414, seq2는 0.5186으로 아이러니하게 **패턴이 없는 seq2가 있는 seq1보다 더 높은 확률**을 보인다.\n",
        "\n",
        "[원인 분석]\n",
        "\n",
        "RNN의 주요 문제점인 **장기 의존성** 때문으로 보인다. seq1의 패턴인 '101'은 시퀀스의 초반부분에만 등장하고 연속으로 '0'이 입력된다. t=2까지 인식한 패턴이 마지막 시점에는 거의 남아있지 않아 낮은 확률이 나타나게 된다.\n",
        "\n",
        "반면 seq2는 '1'과 '0'이 빈번하게 등장해 seq1보다 특성 자체를 패턴으로 오인하여 데이터의 순서가 아닌 **현재의 변동성**만 보고 확률을 낸 것이다."
      ],
      "metadata": {
        "id": "pifQ80I8kZr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Sample code in PyTorch"
      ],
      "metadata": {
        "id": "njt9mJdlkiDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래는 위와 동일하게 진행"
      ],
      "metadata": {
        "id": "G8lrNsxJFRGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "kW-QeXmXkhF9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "XMoSaivQFF7Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "uo-gXIZOFF5Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기서부터 LSTM 모델 클래스"
      ],
      "metadata": {
        "id": "0hzsnnU5FL2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # (0/1) -> 벡터로 변환\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                         # (batch, seq_len, embed_dim)\n",
        "        out, (h_n, c_n) = self.lstm(emb)            # out: (batch, seq_len, hidden_dim)\n",
        "                                                    # h_n: (num_layers, batch, hidden_dim)\n",
        "                                                    # c_n: (num_layers, batch, hidden_dim)\n",
        "\n",
        "        last_h = h_n[-1]                            # (batch, hidden_dim)  마지막 layer의 마지막 hidden\n",
        "        logit = self.fc(last_h)                     # (batch, 1)\n",
        "        return logit, out, (h_n, c_n)\n"
      ],
      "metadata": {
        "id": "FyAeyB5sFF2b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN과의 차이점??\n",
        "\n",
        "1. nn.RNN -> nn.LSTM\n",
        "2. lSTM은 hidden state(h) 말고도 cell state(c)가 추가되었다는 점\n",
        "3. forward 결과에 따른 형태? (output, (h_n, c_n))"
      ],
      "metadata": {
        "id": "OAsdRO6FFUe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 메인 함수 : train()\n",
        "- 내부 로직 step by step 설명:\n",
        "  1. train, val dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model, loss function, optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증 정확도 출력\n",
        "  6. 마지막에 demo seq 하나 넣어서 확률 출력\n",
        "  7. demo seq에서 시간별 hidden state 출력"
      ],
      "metadata": {
        "id": "A5EtTq-8FVOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleLSTMClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # logit을 바로 넣는 BCE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)               # x:(B,12), y:(B,1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)                           # logit:(B,1)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)                  # (B,1)\n",
        "                pred = (prob >= 0.5).float()                 # (B,1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch:02d}] loss={avg_loss:.4f} | val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0] # 패턴 없음 -> 확률 낮아야 함\n",
        "\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "    logit, out_all, (h_n, c_n) = model(x_demo)\n",
        "\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "    print(\"\\n--- DEMO ---\")\n",
        "    print(\"demo_seq:\", demo_seq)\n",
        "    print(f\"pred_prob(pattern=1): {prob:.4f}\")\n",
        "\n",
        "    # 시간별 hidden state 일부 출력\n",
        "    # out_all: (1, seq_len, hidden_dim)  -> time step별 hidden이 들어있음 (LSTM의 output)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (seq_len, hidden_dim)\n",
        "\n",
        "    print(\"\\n[time step별 hidden state 앞 6개 차원만 출력]\")\n",
        "    for t in range(out_all.size(0)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        print(f\"t={t:02d}, x={demo_seq[t]} -> h_t[:6]={h_t}\")\n",
        "\n",
        "    # (참고) 마지막 hidden/cell state도 같이 보기\n",
        "    last_h = h_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    last_c = c_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    print(\"\\n[마지막 state 요약]\")\n",
        "    print(\"last_h[:6] =\", last_h[:6].numpy())\n",
        "    print(\"last_c[:6] =\", last_c[:6].numpy())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "stLhRCNMFF0L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "돌려돌려"
      ],
      "metadata": {
        "id": "1QxJeYuWFcOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSe_IZ-DFFyD",
        "outputId": "cddf1bd1-abbc-4c03-d9fd-46884de190c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] loss=0.5911 | val_acc=0.7480\n",
            "[Epoch 02] loss=0.5186 | val_acc=0.7920\n",
            "[Epoch 03] loss=0.4497 | val_acc=0.8390\n",
            "[Epoch 04] loss=0.2712 | val_acc=0.9640\n",
            "[Epoch 05] loss=0.1237 | val_acc=0.9990\n",
            "\n",
            "--- DEMO ---\n",
            "demo_seq: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "pred_prob(pattern=1): 0.9931\n",
            "\n",
            "[time step별 hidden state 앞 6개 차원만 출력]\n",
            "t=00, x=1 -> h_t[:6]=[-0.25507078 -0.2329384   0.13711463  0.06323551  0.12685719  0.25697005]\n",
            "t=01, x=0 -> h_t[:6]=[-0.311606   -0.20604822  0.3557771  -0.33636045  0.2893263   0.05235714]\n",
            "t=02, x=1 -> h_t[:6]=[-0.5422871  -0.574087    0.41573393  0.52337754  0.5185893   0.60772276]\n",
            "t=03, x=0 -> h_t[:6]=[-0.8271026 -0.71493    0.6598518  0.5096213  0.5303713  0.6794197]\n",
            "t=04, x=0 -> h_t[:6]=[-0.927793   -0.699708    0.50943893  0.84892243 -0.01735253  0.80742013]\n",
            "t=05, x=0 -> h_t[:6]=[-0.95522636 -0.7075229   0.5488019   0.9359291  -0.04336341  0.8757468 ]\n",
            "t=06, x=1 -> h_t[:6]=[-0.9154818  -0.8595699   0.8232351   0.9263731   0.23047754  0.9119817 ]\n",
            "t=07, x=1 -> h_t[:6]=[-0.9277372  -0.9199739   0.9275074   0.9281433   0.49771196  0.9293701 ]\n",
            "t=08, x=0 -> h_t[:6]=[-0.9768986  -0.96684736  0.9438855   0.97621036  0.5328927   0.98521996]\n",
            "t=09, x=0 -> h_t[:6]=[-0.9786166  -0.97396916  0.9445397   0.99107116  0.41171923  0.9898045 ]\n",
            "t=10, x=0 -> h_t[:6]=[-0.9784335  -0.9740374   0.94322354  0.99351025  0.37650305  0.98996156]\n",
            "t=11, x=0 -> h_t[:6]=[-0.9785488  -0.97498244  0.9423881   0.99426615  0.3544078   0.99033487]\n",
            "\n",
            "[마지막 state 요약]\n",
            "last_h[:6] = [-0.9785488  -0.97498244  0.9423881   0.99426615  0.3544078   0.99033487]\n",
            "last_c[:6] = [-5.15419   -3.4453552  2.0604556  4.438824   0.4569346  4.6219993]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN과 비교할 점 :\n",
        "  1. val_acc\n",
        "  2. train_loss\n",
        "  3. demo prob\n",
        "\n",
        "\n",
        "- RNN vs LSTM\n",
        "\n",
        "  현재는 장기기억이 필요 없어서 유사한 상황.\n",
        "  \n",
        "  trade-off 중요성"
      ],
      "metadata": {
        "id": "0XZsF577Fgy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Sample code in PyTorch"
      ],
      "metadata": {
        "id": "XRjTLGz8FkJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "FZQRzd_yFFwH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "IyxVJD_HFFtX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "Py8J1mxyFFq3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GRU 클래스:\n",
        "  nn.GRU\n",
        "  \n",
        "  LSTM처럼 cell state(c)가 없고, hidden state(h) 하나만 유지\n",
        "\n",
        "  forward 결과로 out, h_n\n",
        "  \n",
        "  out : 모든 time step의 hidden (batch, seq_len, hidden_dim)\n",
        "\n",
        "  h_n : 마지막 hidden (num_layers, batch, hidden_dim)"
      ],
      "metadata": {
        "id": "tXUAnZxhFtM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                 # (B, T, E)\n",
        "        out, h_n = self.gru(emb)            # out: (B, T, H), h_n: (1, B, H)\n",
        "        last_h = h_n[-1]                    # (B, H)\n",
        "        logit = self.fc(last_h)             # (B, 1)\n",
        "        return logit, out, h_n"
      ],
      "metadata": {
        "id": "n_7qciOPFFoo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gru():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleGRUClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                pred = (prob >= 0.5).float()\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={avg_loss:.4f}  val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0]  # 패턴 있음\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "\n",
        "    logit, out_all, h_n = model(x_demo)\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(f\"Final prob(pattern=101): {prob:.4f}\")\n",
        "\n",
        "    # time step별 hidden state 출력 (앞 6개 차원)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (T, H)\n",
        "\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    T = out_all.size(0)\n",
        "    for t in list(range(3)) + list(range(T-3, T)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        # 보기 좋게 소수점 3자리로\n",
        "        h_t_fmt = [float(f\"{v:.3f}\") for v in h_t]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={h_t_fmt}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nrZXy-8iFFlr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = train_gru()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARHGR8JkFFjY",
        "outputId": "001d02c6-9279-459e-ebb3-c9f58439937d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5617  val_acc=0.7220\n",
            "[Epoch 2] train_loss=0.4833  val_acc=0.8420\n",
            "[Epoch 3] train_loss=0.1750  val_acc=0.9980\n",
            "[Epoch 4] train_loss=0.0436  val_acc=1.0000\n",
            "[Epoch 5] train_loss=0.0215  val_acc=1.0000\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9967\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[0.824, -0.197, -0.234, 0.305, 0.11, 0.266]\n",
            "t= 1, x_t=0 -> h_t[:6]=[-0.254, -0.007, -0.007, 0.07, 0.006, 0.176]\n",
            "t= 2, x_t=1 -> h_t[:6]=[0.824, -0.861, -0.545, 0.772, -0.021, 0.625]\n",
            "t= 9, x_t=0 -> h_t[:6]=[0.966, -0.97, -0.91, 0.8, -0.978, 0.818]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.967, -0.969, -0.906, 0.771, -0.977, 0.834]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.968, -0.968, -0.901, 0.743, -0.976, 0.847]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM vs GRU ?"
      ],
      "metadata": {
        "id": "lnW68WglF24v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM & GRU 과제"
      ],
      "metadata": {
        "id": "gmuBRGxgF5ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 장기기억 성능을 비교하기 위한 코드입니다.\n",
        "\n",
        "코드 중간의 빈칸을 채우면서, 매애앤 아래의 답변을 채워주시면 됩니다.\n",
        "\n",
        "모르면 인공지능을 사용해도 좋지만, sample code로도 풀 수 있으니 최대한 본인의 힘으로 해보면 좋겠습니다 !!"
      ],
      "metadata": {
        "id": "zM42wRXOF8Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "bItDmp57FyrU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tmi) 11/7은 제 생일입니다. 감사합니다.\n",
        "def set_seed(seed=117):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(117)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QkVj3qZuFyqA",
        "outputId": "06985199-2714-4e83-824d-433fc93d2136"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LongMemoryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_samples=8000, T=80):\n",
        "        self.T = T\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            first_bit = random.randint(0, 1)          # 기억해야 할 정보\n",
        "            middle = [random.randint(0, 1) for _ in range(T-2)]\n",
        "            seq = [first_bit] + middle + [2]          # 마지막은 DELIM=2\n",
        "            label = first_bit\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)               # (T,)\n",
        "        y = torch.tensor([label], dtype=torch.float)          # (1,)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "hZrbG1IeFyoJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. lstm 모델 빈칸 채우기"
      ],
      "metadata": {
        "id": "iLi1JgS9GFQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어를 선언하세요.\n",
        "        self.embed = nn.Embedding(vocab_size,embed_dim)\n",
        "        # TODO : LSTM 레이어를 선언하세요. (batch_first=True)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = embed_dim,\n",
        "            hidden_size = hidden_dim,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO: 임베딩을 통과시키세요.\n",
        "        emb = self.embed(x)\n",
        "        # TODO : LSTM에 넣고 out, (h_n, c_n)을 받으세요.\n",
        "        out, (h_n, c_n) = self.lstm(emb)\n",
        "        # TODO : 마지막 hidden(last_h)을 얻으세요.\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, (h_n, c_n)\n"
      ],
      "metadata": {
        "id": "HWz4-GNaFymh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. gru 모델 빈칸 채우기"
      ],
      "metadata": {
        "id": "U3dXxnYlGGih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : GRU 레이어 (batch_first=True)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size= embed_dim,\n",
        "            hidden_size= hidden_dim,\n",
        "            batch_first= True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO : 임베딩\n",
        "        emb = self.embed(x)\n",
        "        # TODO : GRU forward로 out, h_n 받기\n",
        "        out, h_n = self.gru(emb)\n",
        "        # TODO : 마지막 hidden\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, h_n\n"
      ],
      "metadata": {
        "id": "SXYh1847Fyko"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 루프 만들기"
      ],
      "metadata": {
        "id": "ogOE47LuGP7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import optimizer\n",
        "def train_model(model, train_loader, val_loader, epochs=6, lr=1e-3, device=\"cpu\", tag=\"\"):\n",
        "    model = model.to(device)\n",
        "    # TODO : loss 함수 선언 (BCEWithLogitsLoss)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    # TODO : optimizer 선언 (Adam)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # TODO : gradient 초기화\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "            # TODO : loss 계산\n",
        "            loss = crit(logit, y)\n",
        "            # TODO : backprop\n",
        "            loss.backward()\n",
        "            # TODO : optimizer step\n",
        "            opt.step()\n",
        "\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "                # TODO : prob = sigmoid(logit)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                # TODO : pred = (prob >= 0.5)\n",
        "                pred = (prob >= 0.5).float()\n",
        "\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"{tag}[Epoch {ep}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ELMItPqMGRG2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그대로 실행하시면 됩니다.\n",
        "\n",
        "T = 300\n",
        "train_ds = LongMemoryDataset(n_samples=8000, T=T)\n",
        "val_ds   = LongMemoryDataset(n_samples=2000, T=T)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "lstm = LSTMClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "gru  = GRUClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "\n",
        "print(\"=== LSTM ===\")\n",
        "lstm = train_model(lstm, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"LSTM \")\n",
        "\n",
        "print(\"\\n=== GRU ===\")\n",
        "gru = train_model(gru, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"GRU  \")"
      ],
      "metadata": {
        "id": "MqgsERicGRFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8023bd52-acf5-4524-ed21-c505ef9b1f4c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LSTM ===\n",
            "LSTM [Epoch 1] train_loss=0.6932  val_acc=0.5095\n",
            "LSTM [Epoch 2] train_loss=0.6931  val_acc=0.4990\n",
            "LSTM [Epoch 3] train_loss=0.6929  val_acc=0.5210\n",
            "LSTM [Epoch 4] train_loss=0.6931  val_acc=0.5210\n",
            "LSTM [Epoch 5] train_loss=0.6932  val_acc=0.5210\n",
            "LSTM [Epoch 6] train_loss=0.6929  val_acc=0.5210\n",
            "\n",
            "=== GRU ===\n",
            "GRU  [Epoch 1] train_loss=0.6929  val_acc=0.5050\n",
            "GRU  [Epoch 2] train_loss=0.6940  val_acc=0.5210\n",
            "GRU  [Epoch 3] train_loss=0.6932  val_acc=0.5210\n",
            "GRU  [Epoch 4] train_loss=0.6932  val_acc=0.4775\n",
            "GRU  [Epoch 5] train_loss=0.6930  val_acc=0.4935\n",
            "GRU  [Epoch 6] train_loss=0.6930  val_acc=0.5210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. LSTM과 GRU의 차이점에 대해서 간략하게 서술해주세요."
      ],
      "metadata": {
        "id": "M8o9PlFmGZMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)\n",
        "\n",
        "LSTM은 삭제, 입력, 출력의 3가지 gate로 이루어져있지만 GRU는 복잡한 구조를 단순화해 효율성을 높이기 위해 삭제와 입력 gate를 합친 2가지 gate로 이루어져 있다."
      ],
      "metadata": {
        "id": "SfE8A2EFGeHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. T=80에서 LSTM과 GRU의 학습 곡선을 비교하고, 어느 쪽이 더 안정적으로 수렴했는지 서술해주세요."
      ],
      "metadata": {
        "id": "PRfrs3zGGiDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)\n",
        "\n",
        "LSTM이 더 안정적으로 수렴했다. Loss값이 0.6934 -> 0.6932로 큰 변화가 없으며, 정확도가 대부분 0.5065에 머물러 있다."
      ],
      "metadata": {
        "id": "IjAVyZcEGiBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. T를 80 → 150 → 300 순으로 늘려서 각각 실행해보고, 어떤 모델이 성능을 더 잘 유지하는지, 왜 그런 것 같은지를 서술해주세요."
      ],
      "metadata": {
        "id": "nWhHz27cGiAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)\n",
        "\n",
        "[80]\n",
        "\n",
        "```\n",
        "=== LSTM ===\n",
        "LSTM [Epoch 1] train_loss=0.6934  val_acc=0.4925\n",
        "LSTM [Epoch 2] train_loss=0.6931  val_acc=0.5065\n",
        "LSTM [Epoch 3] train_loss=0.6932  val_acc=0.5065\n",
        "LSTM [Epoch 4] train_loss=0.6934  val_acc=0.5065\n",
        "LSTM [Epoch 5] train_loss=0.6932  val_acc=0.5065\n",
        "LSTM [Epoch 6] train_loss=0.6932  val_acc=0.5065\n",
        "\n",
        "=== GRU ===\n",
        "GRU  [Epoch 1] train_loss=0.6936  val_acc=0.5065\n",
        "GRU  [Epoch 2] train_loss=0.6932  val_acc=0.4935\n",
        "GRU  [Epoch 3] train_loss=0.6936  val_acc=0.4930\n",
        "GRU  [Epoch 4] train_loss=0.6932  val_acc=0.4995\n",
        "GRU  [Epoch 5] train_loss=0.6932  val_acc=0.5065\n",
        "GRU  [Epoch 6] train_loss=0.6933  val_acc=0.4975\n",
        "```\n",
        "\n",
        "[150]\n",
        "\n",
        "```\n",
        "=== LSTM ===\n",
        "LSTM [Epoch 1] train_loss=0.6937  val_acc=0.5175\n",
        "LSTM [Epoch 2] train_loss=0.6932  val_acc=0.5175\n",
        "LSTM [Epoch 3] train_loss=0.6931  val_acc=0.5175\n",
        "LSTM [Epoch 4] train_loss=0.6934  val_acc=0.5175\n",
        "LSTM [Epoch 5] train_loss=0.6933  val_acc=0.5175\n",
        "LSTM [Epoch 6] train_loss=0.6932  val_acc=0.5175\n",
        "\n",
        "=== GRU ===\n",
        "GRU  [Epoch 1] train_loss=0.6935  val_acc=0.5175\n",
        "GRU  [Epoch 2] train_loss=0.6934  val_acc=0.5175\n",
        "GRU  [Epoch 3] train_loss=0.6935  val_acc=0.5175\n",
        "GRU  [Epoch 4] train_loss=0.6933  val_acc=0.5175\n",
        "GRU  [Epoch 5] train_loss=0.6932  val_acc=0.5175\n",
        "GRU  [Epoch 6] train_loss=0.6932  val_acc=0.5175\n",
        "```\n",
        "\n",
        "[300]\n",
        "\n",
        "```\n",
        "=== LSTM ===\n",
        "LSTM [Epoch 1] train_loss=0.6932  val_acc=0.5095\n",
        "LSTM [Epoch 2] train_loss=0.6931  val_acc=0.4990\n",
        "LSTM [Epoch 3] train_loss=0.6929  val_acc=0.5210\n",
        "LSTM [Epoch 4] train_loss=0.6931  val_acc=0.5210\n",
        "LSTM [Epoch 5] train_loss=0.6932  val_acc=0.5210\n",
        "LSTM [Epoch 6] train_loss=0.6929  val_acc=0.5210\n",
        "\n",
        "=== GRU ===\n",
        "GRU  [Epoch 1] train_loss=0.6929  val_acc=0.5050\n",
        "GRU  [Epoch 2] train_loss=0.6940  val_acc=0.5210\n",
        "GRU  [Epoch 3] train_loss=0.6932  val_acc=0.5210\n",
        "GRU  [Epoch 4] train_loss=0.6932  val_acc=0.4775\n",
        "GRU  [Epoch 5] train_loss=0.6930  val_acc=0.4935\n",
        "GRU  [Epoch 6] train_loss=0.6930  val_acc=0.5210\n",
        "```\n",
        "\n",
        "[성능 비교]\n",
        "\n",
        "LSTM이 GRU보다 loss값과 정확도의 안정성의 측면에서 성능을 더 잘 유지하고 있다. 그러나 두 모델 모두 T값을 80-> 150 -> 300으로 늘릴수록 점차 학습 능력을 상실하고 성능이 낮아짐을 볼 수 있다.\n",
        "\n",
        "특히 T=150일 때 정확도가 0.5175에 그대로 머무르는 것을 보아, 기울기 소멸 문제로 가중치가 업데이트 되지 않음을 알 수 있다.\n",
        "즉 T값이 커질수록 모델 성능에 비해 지나치게 길어지는 시퀀스에 따라 기울기 업데이트가 원활하게 진행되지 않아 발생하는 문제라 생각한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "OpLgdVqOGh66"
      }
    }
  ]
}